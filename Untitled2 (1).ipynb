{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf2e37e-249b-4480-a331-66da673e6a3d",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "\n",
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?\n",
    "Elastic Net Regression is a regularized regression technique that combines the penalties of both Lasso (Least Absolute Shrinkage and Selection Operator) and Ridge Regression. It includes both the \n",
    "ğ¿\n",
    "1\n",
    "L \n",
    "1\n",
    "â€‹\n",
    "  (absolute value) and \n",
    "ğ¿\n",
    "2\n",
    "L \n",
    "2\n",
    "â€‹\n",
    "  (squared value) penalties, controlled by two hyperparameters.\n",
    "\n",
    "Elastic Net Regression Equation:\n",
    "Loss\n",
    "=\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    ")\n",
    "2\n",
    "+\n",
    "ğœ†\n",
    "1\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "âˆ£\n",
    "ğ›½\n",
    "ğ‘—\n",
    "âˆ£\n",
    "+\n",
    "ğœ†\n",
    "2\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "2\n",
    "Loss=âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " ) \n",
    "2\n",
    " +Î» \n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " âˆ£Î² \n",
    "j\n",
    "â€‹\n",
    " âˆ£+Î» \n",
    "2\n",
    "â€‹\n",
    " âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "2\n",
    "â€‹\n",
    " \n",
    "where:\n",
    "\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "y \n",
    "i\n",
    "â€‹\n",
    "  are the observed values.\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    "  are the predicted values.\n",
    "ğ›½\n",
    "ğ‘—\n",
    "Î² \n",
    "j\n",
    "â€‹\n",
    "  are the coefficients.\n",
    "ğœ†\n",
    "1\n",
    "Î» \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğœ†\n",
    "2\n",
    "Î» \n",
    "2\n",
    "â€‹\n",
    "  are the regularization parameters for the Lasso and Ridge penalties, respectively.\n",
    "Differences from Other Regression Techniques:\n",
    "\n",
    "OLS Regression: Minimizes the sum of squared residuals without any regularization.\n",
    "Ridge Regression: Adds an \n",
    "ğ¿\n",
    "2\n",
    "L \n",
    "2\n",
    "â€‹\n",
    "  penalty (squared coefficients) to the loss function, which shrinks coefficients but does not set them to zero.\n",
    "Lasso Regression: Adds an \n",
    "ğ¿\n",
    "1\n",
    "L \n",
    "1\n",
    "â€‹\n",
    "  penalty (absolute coefficients) to the loss function, which can shrink coefficients to zero, performing feature selection.\n",
    "Elastic Net Regression: Combines both \n",
    "ğ¿\n",
    "1\n",
    "L \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğ¿\n",
    "2\n",
    "L \n",
    "2\n",
    "â€‹\n",
    "  penalties, providing a compromise between Ridge and Lasso, useful for handling correlated predictors and performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b523c-3fec-42ce-81fb-c3ef7bdf5cfa",
   "metadata": {},
   "source": [
    "Q2\n",
    "The optimal values of the regularization parameters \n",
    "ğœ†\n",
    "1\n",
    "Î» \n",
    "1\n",
    "â€‹\n",
    "  (Lasso penalty) and \n",
    "ğœ†\n",
    "2\n",
    "Î» \n",
    "2\n",
    "â€‹\n",
    "  (Ridge penalty) can be chosen using techniques such as:\n",
    "\n",
    "Cross-Validation: Use cross-validation to evaluate the model with different combinations of \n",
    "ğœ†\n",
    "1\n",
    "Î» \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğœ†\n",
    "2\n",
    "Î» \n",
    "2\n",
    "â€‹\n",
    "  values, and select the combination that minimizes the cross-validation error.\n",
    "Grid Search: Specify a grid of possible values for \n",
    "ğœ†\n",
    "1\n",
    "Î» \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğœ†\n",
    "2\n",
    "Î» \n",
    "2\n",
    "â€‹\n",
    " , and use cross-validation to find the optimal pair within this grid.\n",
    "Random Search: Randomly sample combinations of \n",
    "ğœ†\n",
    "1\n",
    "Î» \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğœ†\n",
    "2\n",
    "Î» \n",
    "2\n",
    "â€‹\n",
    "  values and use cross-validation to find the best combination.\n",
    "Elastic Net Path: Plot the coefficients as a function of the regularization parameters and select the values that balance model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401852d-6b45-4a5d-928b-5d99e1c8dcee",
   "metadata": {},
   "source": [
    "Q3\n",
    "Advantages:\n",
    "\n",
    "Combines Benefits: Combines the benefits of both Lasso (feature selection) and Ridge (handling multicollinearity) regressions.\n",
    "Flexible: Provides a more flexible regularization approach by adjusting both \n",
    "ğ¿\n",
    "1\n",
    "L \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğ¿\n",
    "2\n",
    "L \n",
    "2\n",
    "â€‹\n",
    "  penalties.\n",
    "Robust to Multicollinearity: Performs well in the presence of highly correlated predictors.\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: More complex than Ridge or Lasso alone due to the need to tune two regularization parameters.\n",
    "Interpretation: Interpretation of the model can be more difficult compared to simpler regression techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466eeac3-eea0-4402-b4fb-bc4b8c4544c1",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "Common Use Cases:\n",
    "\n",
    "High-Dimensional Data: When the number of predictors is much larger than the number of observations.\n",
    "Feature Selection: When feature selection is important, but the predictors are highly correlated.\n",
    "Genomic Data: In bioinformatics and genomics, where datasets often contain many correlated features.\n",
    "Econometrics: In economic modeling where predictors are often correlated and feature selection is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c268eb9-350a-4d2e-9222-360005f51b97",
   "metadata": {},
   "source": [
    "Q5\n",
    "Interpreting the coefficients in Elastic Net Regression is similar to interpreting coefficients in Ridge and Lasso regressions, with some considerations for the regularization effects:\n",
    "\n",
    "Magnitude: The coefficients represent the change in the response variable for a one-unit change in the predictor, holding other predictors constant. Regularization shrinks the coefficients, so their magnitudes are typically smaller than in OLS regression.\n",
    "Direction: The sign of the coefficients (positive or negative) indicates the direction of the relationship between the predictor and the response variable.\n",
    "Feature Selection: Non-zero coefficients indicate selected important features, while coefficients shrunk to zero indicate excluded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0378f-9dc7-4ee2-8f45-e9c9ff7ec0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
